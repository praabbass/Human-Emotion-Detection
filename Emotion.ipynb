{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f3bad48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset Loaded: (2880, 193), 2880 samples\n",
      "ðŸŽ¯ Accuracy: 0.9131944444444444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.97      0.95      0.96        76\n",
      "        calm       0.86      0.97      0.91        77\n",
      "     disgust       0.81      0.79      0.80        77\n",
      "     fearful       0.95      0.90      0.92        77\n",
      "       happy       0.92      0.92      0.92        77\n",
      "     neutral       1.00      1.00      1.00        38\n",
      "         sad       0.92      0.87      0.89        77\n",
      "   surprised       0.92      0.95      0.94        77\n",
      "\n",
      "    accuracy                           0.91       576\n",
      "   macro avg       0.92      0.92      0.92       576\n",
      "weighted avg       0.91      0.91      0.91       576\n",
      "\n",
      "âœ… Model, encoder, and scaler saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Path to your extracted dataset\n",
    "DATA_PATH = r\"C:\\praabhass\\python\\Task 3\\Audio actor\"\n",
    "\n",
    "# Emotion mapping from the file name\n",
    "emotion_map = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "# Feature extractor\n",
    "def extract_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None, res_type=\"soxr_hq\")\n",
    "\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr).T, axis=0)\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr).T, axis=0)\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr).T, axis=0)\n",
    "\n",
    "    return np.hstack([mfccs, chroma, mel, contrast, tonnetz])\n",
    "\n",
    "# Load dataset\n",
    "X, y = [], []\n",
    "for root, dirs, files in os.walk(DATA_PATH):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):\n",
    "            path = os.path.join(root, file)\n",
    "            try:\n",
    "                features = extract_features(path)\n",
    "                parts = file.split(\"-\")  # For example: \"03-01-01-01-01-01-01.wav\"\n",
    "                if len(parts) > 2:\n",
    "                    label_code = parts[2]\n",
    "                    emotion = emotion_map.get(label_code, None)\n",
    "                    if emotion:\n",
    "                        X.append(features)\n",
    "                        y.append(emotion)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(f\"âœ… Dataset Loaded: {X.shape}, {len(y)} samples\")\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Train Random Forest (you can switch to SVM if desired)\n",
    "model = RandomForestClassifier(n_estimators=500, max_depth=50, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"ðŸŽ¯ Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred, target_names=encoder.classes_))\n",
    "\n",
    "# Save model, encoder, scaler\n",
    "joblib.dump(model, \"audio_emotion_model.pkl\")\n",
    "joblib.dump(encoder, \"audio_label_encoder.pkl\")\n",
    "joblib.dump(scaler, \"audio_scaler.pkl\")\n",
    "print(\"âœ… Model, encoder, and scaler saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86b8889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import librosa\n",
    "import joblib\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import tempfile\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import base64\n",
    "\n",
    "# === Constants ===\n",
    "IMG_SIZE = 224\n",
    "MAX_RECORD_SECONDS = 20\n",
    "\n",
    "# === Load Pre-trained Models ===\n",
    "audio_model = joblib.load(\"audio_emotion_model.pkl\")\n",
    "audio_encoder = joblib.load(\"audio_label_encoder.pkl\")\n",
    "audio_scaler = joblib.load(\"audio_scaler.pkl\")\n",
    "\n",
    "# === Emotion Classes ===\n",
    "audio_classes = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
    "\n",
    "# === Helper Functions ===\n",
    "def extract_audio_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None, res_type=\"soxr_hq\")\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr).T, axis=0)\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr).T, axis=0)\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr).T, axis=0)\n",
    "    return np.hstack([mfccs, chroma, mel, contrast, tonnetz])\n",
    "\n",
    "def predict_audio(file_path):\n",
    "    features = extract_audio_features(file_path)\n",
    "    features = audio_scaler.transform([features])\n",
    "    pred = audio_model.predict(features)\n",
    "    emotion = audio_encoder.inverse_transform(pred)[0]\n",
    "    return emotion\n",
    "\n",
    "def record_audio(duration=20, fs=44100):\n",
    "    st.info(f\"Recording for {duration} seconds...\")\n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "    sd.wait()\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')\n",
    "    sf.write(temp_file.name, recording, fs)\n",
    "    return temp_file.name\n",
    "\n",
    "# === Streamlit App Configuration ===\n",
    "st.set_page_config(page_title=\"ðŸŽ§ Audio Emotion Detector\", layout=\"wide\")\n",
    "\n",
    "# Background Image CSS\n",
    "def set_background(image_path):\n",
    "    with open(image_path, \"rb\") as img:\n",
    "        encoded = base64.b64encode(img.read()).decode()\n",
    "    st.markdown(\n",
    "        f\"\"\"\n",
    "        <style>\n",
    "        .stApp {{\n",
    "            background-image: url(\"data:image/png;base64,{encoded}\");\n",
    "            background-size: cover;\n",
    "            background-position: center;\n",
    "            background-repeat: no-repeat;\n",
    "            background-attachment: fixed;\n",
    "            color: white;\n",
    "        }}\n",
    "        .stApp::before {{\n",
    "            content: \"\";\n",
    "            position: fixed;\n",
    "            top: 0;\n",
    "            left: 0;\n",
    "            width: 100%;\n",
    "            height: 100%;\n",
    "            background: rgba(0, 0, 0, 0.6); /* Overlay */\n",
    "            z-index: -1;\n",
    "        }}\n",
    "        .css-18e3th9 {{\n",
    "            padding-top: 5rem;\n",
    "            padding-bottom: 5rem;\n",
    "        }}\n",
    "        </style>\n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True\n",
    "    )\n",
    "\n",
    "# Set the background image\n",
    "set_background(r'C:\\praabhass\\python\\Task 3\\Gemini_Generated_Image_qo1cb6qo1cb6qo1c.png')\n",
    "\n",
    "# App content\n",
    "st.title(\"ðŸŽ§ Audio Emotion Recognition\")\n",
    "\n",
    "option = st.radio(\"Choose Input Method\", [\"Upload Audio\", \"Record Live Audio\"])\n",
    "\n",
    "if option == \"Upload Audio\":\n",
    "    audio_file = st.file_uploader(\"Upload your audio file (WAV)\", type=['wav'])\n",
    "    if audio_file is not None:\n",
    "        st.audio(audio_file, format='audio/wav')\n",
    "        if st.button(\"Proceed\"):\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp:\n",
    "                tmp.write(audio_file.read())\n",
    "                tmp_path = tmp.name\n",
    "            emotion = predict_audio(tmp_path)\n",
    "            st.success(f\"ðŸŽ¯ Predicted Emotion: {emotion}\")\n",
    "\n",
    "elif option == \"Record Live Audio\":\n",
    "    duration = st.slider(\"Recording Duration (seconds)\", min_value=5, max_value=20, value=10)\n",
    "    if st.button(\"Record Audio\"):\n",
    "        recorded_path = record_audio(duration=duration)\n",
    "        st.audio(recorded_path, format='audio/wav')\n",
    "        emotion = predict_audio(recorded_path)\n",
    "        st.success(f\"ðŸŽ¯ Predicted Emotion: {emotion}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
